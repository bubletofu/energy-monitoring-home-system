# Process of Data Compression in "Dynamic Online Performance Optimization in Streaming Data Compression"

## 1. Overview of the Algorithm
The paper presents an algorithm that optimizes performance in streaming data compression by dynamically adjusting compression parameters in real-time. Instead of using a fixed compression level, the algorithm continuously evaluates and adapts settings to maximize efficiency.

### Key Mechanisms of the Algorithm:
- **Adaptive Compression Model:** Adjusts compression techniques based on the characteristics of incoming data.
- **Dynamic Parameter Optimization:** Continuously optimizes compression parameters instead of using fixed values.
- **Feedback Loop:** Uses a feedback mechanism to update the compression model based on past performance.

---

## 2. Formulas and Optimization Methods
The algorithm employs mathematical models to optimize compression efficiency:

### **Compression Error Rate (CER):**
\[
CER = \frac{|X - \hat{X}|}{X}
\]
Where:
- \(X\) is the original data.
- \(\hat{X}\) is the decompressed data.

### **Cost Function (CF):**
\[
CF = w_1 \cdot CER + w_2 \cdot CR
\]
Where:
- **Compression Ratio (CR):**
  \[
  CR = \frac{\text{Size of Original Data}}{\text{Size of Compressed Data}}
  \]
- \(w_1, w_2\) are weight factors balancing accuracy and compression efficiency.

### **Optimization Methods:**
The algorithm employs:
- **Gradient Descent** for parameter tuning.
- **Genetic Algorithms (GA)** as a heuristic approach for optimization.

---

## 3. Example of Data Compression
Consider a real-time IoT temperature sensor streaming data every second:

### **Original Data Sample:**
[24.01, 24.03, 24.04, 24.07, 24.08, 24.10, 24.12, 24.14, 24.16, 24.18]

### **Using Dynamic Compression Algorithm:**
- If the data exhibits **minor variations**, Huffman Coding or Run-Length Encoding (RLE) may be applied.
- If the data fluctuates significantly, **Wavelet Compression** or **Principal Component Analysis (PCA)** may be preferred.

### **Compressed Data (Example using Huffman Coding):**
```
1011100110001110110
```

### **Decompressed Data (with slight error):**
[24.01, 24.03, 24.05, 24.08, 24.10, 24.12, 24.15, 24.18]

### **Compression Error Rate (CER):**
\[
CER = \frac{|24.04 - 24.05|}{24.04} \approx 0.0042
\]

### **Compression Ratio (CR):**
Assuming the original data size is **80 bits** (10 float values, each 8 bits), and the compressed data size is **19 bits**:
\[
CR = \frac{80}{19} \approx 4.21
\]

### **Outcome:**
If excessive data loss occurs, the system dynamically adjusts the compression settings.

---

## 4. Block Size Selection Using Multistage Random Sampling
The algorithm dynamically selects the optimal block size \(n\) for compression using random sampling and curve fitting.

### **Algorithm Steps:**
1. **Initialize parameters** and wait for incoming data.
2. Compare the current data sample \(X_i\) with stored templates \(\Theta_j\) in the buffer.
3. Increment trial count \(r\).
4. If \(k < k_{max}\) (switch count within limits) and \(r \geq r_{min}\) (minimum trials reached):
   - Fit a **2D polynomial curve** using the best-performing block sizes \(n_{best} \pm wc\).
   - Compute optimal block size \(n_{new}\):
     \[
     n_{new} = \arg\max \{\text{curve}\}
     \]
   - If \(n_{new}\) is within the denial window \((n - wn \leq n_{new} \leq n + wn)\), update \(n\), reset the buffer, and restart trials.
   - Otherwise, exit the loop.
5. Increment switch count \(k\) and repeat until convergence.

### **Mathematical Formulation:**
\[
\lim_{r\to\infty} \frac{8n + 8nr}{1+(1+8n)+h+(1+8n)(r−h)} = \frac{8n}{1+8n−8np̂}
\]
Where:
- \(p̂\) is the estimated probability of hitting an optimal compression block.
- \(h\) is an adaptive factor adjusting block transitions.

---

## 5. Example of Block Size Selection in Streaming Data Compression
Consider an IoT device transmitting sensor data every 5 minutes.

### **Example Data Stream:**
```
ID: 1, Device: sensor_001, Data: { "power": 71.312, "humidity": 45.111, "pressure": 1009.625, "temperature": 31.72 }, Timestamp: 2025-06-21 06:35:00
ID: 2, Device: sensor_001, Data: { "power": 71.450, "humidity": 45.200, "pressure": 1009.700, "temperature": 31.75 }, Timestamp: 2025-06-21 06:40:00
```

### **Processing Steps:**
1. **Templates are created** from historical sensor data patterns.
2. **New data is compared** with existing templates to determine reusability.
3. **Kolmogorov-Smirnov Test (KS-Test)** checks if the new data distribution is significantly different.
4. **Block size dynamically adjusts** based on template hit ratio and curve fitting.
5. If the template is valid, **reuse it**; otherwise, **generate a new template**.
6. **Compressed data is stored** with references to the original dataset.

### **Storage After Compression:**
Instead of storing raw sensor data, only compressed templates and references are stored:

| ID  | Template ID | Compressed Data   | Reference       |
|-----|------------|------------------|----------------|
| 1   | T1001      | 1100110010010    | Original Data  |
| 2   | T1001      | 1100110010011    | Original Data  |
| 3   | T1023      | 1011010001101    | Original Data  |

---

## 6. Summary of the Algorithm
- The algorithm dynamically adjusts block size \(n\) using:
  - **Random sampling** to explore new block sizes.
  - **Curve fitting** to predict the best-performing block size.
  - **KS-Test** to verify template validity.
- **Compressed data is stored as template references**, reducing storage overhead.
- The approach allows for **efficient storage while maintaining data integrity**.

